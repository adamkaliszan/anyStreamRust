{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor flow and other basic stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplary item in data frame:\n",
    "```json\n",
    "{ \n",
    "    \"system\" : {\n",
    "        \"class\" : { \"a\" : 8.0, \"mu\" : 1.0, \"arrival_stream_type\" : \"Uniform\", \"arrival_e2d2\" : 3.0, \"service_stream_type\" : \"Poisson\", \"service_e2d2\" : 1.0 },\n",
    "        \"v\" : 1 \n",
    "    },\n",
    "    \"stat\" : { \n",
    "        \"states\" : [ \n",
    "            { \"p\" : 0.07650509235411221, \"out_new\" : 12.249978258197231, \"out_end\" : 0.0 },\n",
    "            { \"p\" : 0.9234949076458878, \"out_new\" : 7.8230198889517375, \"out_end\" : 1.0148249981889534 } ],\n",
    "        \"v\" : 1,\n",
    "        \"no_of_events\" : 18458,\n",
    "        \"metadata\" : { \n",
    "            \"min_no_of_events_per_state\" : 103, \n",
    "            \"uuid\" : { \"$binary\" : \"KAEP7WQWRQq6ixFm2z+Zzg==\", \"$type\" : \"04\" },\n",
    "            \"version\" : \"0.3.0\" } \n",
    "    } \n",
    "}\n",
    "```\n",
    "where:\n",
    "- system is the system description. Traffic class parameters and systems capacity\n",
    "- stat are single simulation statistics. There are many series of such simulation that should be averaged. Is one series is significantly different, it should be dropped. \n",
    "  - each stat has V+1 states.\n",
    "  - each state has its probability and passage intensities (out_new and out_end).\n",
    "  - each stat has metadata:\n",
    "    - min_no_of_events defines how log is the simulation. Increasing this value increases the simulation quality. During processing the stats, statistics with value below given threshold should be ignored and dropped\n",
    "    - unique UUID tat can be used for discrimination of wrong statistics\n",
    "    - version is used in case of bug detection. After fixing the bug, the version number is increased.\n",
    "\n",
    "The main goal is to obtain out_new and out_end using machine learning. Base on distribution for system with V capacity we want to obtain parameters for systems with capacities 1, 2, ..., V-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import math\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "def _connect_mongo(host, port, username, password, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "\n",
    "\n",
    "    return conn[db]\n",
    "\n",
    "\n",
    "def read_mongo(db=\"anystream\", collection=\"statistics\", query={}, host='mongo.adamkaliszan.pl', port=27017, username='anonymus', password='password', no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n",
    "\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df\n",
    "\n",
    "def getResults(df, v=0, **kwargs):\n",
    "    dfF = df.loc[df.system.map(lambda x: x['v']) == v]\n",
    "\n",
    "    for key in kwargs:\n",
    "        dfF = dfF.loc[dfF.system.map(lambda x: x['class'][key]) == kwargs[key]]\n",
    "\n",
    "    return dfF\n",
    "\n",
    "def getClasses(df, v = 1, **kwargs):            \n",
    "    if v > 0:\n",
    "        dfF = df.loc[df.system.map(lambda x: x['v']) == v]\n",
    "\n",
    "    for key in kwargs:\n",
    "        dfF = dfF.loc[dfF.system.map(lambda x: x['class'][key]) == kwargs[key]]\n",
    "\n",
    "    return dfF.system.drop_duplicates().map(lambda x: x['class']).reset_index(drop=True).to_frame()\n",
    "\n",
    "def getStatistics(df, tr_class, v, params):\n",
    "    dfF = df.loc[df.system.map(lambda x: (x['v'] == v and x['class'] == tr_class))]\n",
    "    dfF = dfF.stat.drop_duplicates().map(lambda x: x['states']).reset_index()\n",
    "    dfF = dfF.drop(columns=['index'])\n",
    "\n",
    "    columns = []\n",
    "\n",
    "    for name in params.values():\n",
    "        for i in range(0, v+1):\n",
    "            columns.append(f\"{name}({i})_{v}\")\n",
    "\n",
    "    values = []\n",
    "\n",
    "    for idx, item in dfF.iterrows():\n",
    "        val_item = []\n",
    "        for par in params:\n",
    "            for i in range(0, v+1):\n",
    "                val_item.append(item.stat[i][par])\n",
    "        values.append(val_item)\n",
    "\n",
    "    dfS = pd.DataFrame(data = values, columns = columns)\n",
    "    return dfS\n",
    "\n",
    "\n",
    "def calculateAvarages(series):\n",
    "    result = series.mean(axis=0)\n",
    "    return result.to_frame().T\n",
    "\n",
    "def dropClassesExact(dfCl, **kwargs):\n",
    "    dfRes = dfCl\n",
    "    for key, value in kwargs.items():\n",
    "        dfRes = dfCl.loc[dfRes.system.map(lambda x: x[key] != value)]\n",
    "    return dfRes.reset_index(drop=True)\n",
    "\n",
    "def dropClassesLess(dfCl, **kwargs):\n",
    "    dfRes = dfCl\n",
    "    for key, value in kwargs.items():\n",
    "        dfRes = dfRes.loc[dfRes.system.map(lambda x: x[key] < value)]\n",
    "    return dfRes.reset_index(drop=True)\n",
    "\n",
    "def dropClassesGreater(dfCl, **kwargs):\n",
    "    dfRes = dfCl\n",
    "    for key, value in kwargs.items():\n",
    "        dfRes = dfRes.loc[dfRes.system.map(lambda x: x[key] > value)]\n",
    "    return dfRes.reset_index(drop=True)\n",
    "\n",
    "def prepareResults(V, dfClasses, df):\n",
    "    params = {\"p\": \"p\", \"out_new\" : \"a\", \"out_end\" : \"s\"}\n",
    "    ClColumns= [\"a\", \"arrival_stream_type\", \"arrival_e2d2\", \"service_stream_type\", \"service_e2d2\"]\n",
    "    dfFinalItems = []\n",
    "    for idx, trClass in enumerate(dfClasses.squeeze()):\n",
    "        clData = [[trClass[\"a\"],\n",
    "                trClass[\"arrival_stream_type\"],\n",
    "                trClass[\"arrival_e2d2\"],\n",
    "                trClass[\"service_stream_type\"],\n",
    "                trClass[\"service_e2d2\"]]]\n",
    "        dfClass = pd.DataFrame(columns=ClColumns, data = clData)\n",
    "        avgStats = [dfClass]\n",
    "        for par, name in params.items():\n",
    "            for v in range (1, V+1):\n",
    "                series = getStatistics(df, trClass, v, {par:name})\n",
    "                if len(series) < 5:\n",
    "                    print(f\"skipping class {trClass}, not enough series (only {len(series)}) for system with capacity {v}\")\n",
    "                    break;\n",
    "                avgStats.append(calculateAvarages(series))\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            dfM = pd.concat(avgStats, axis=1).reset_index(drop=True)\n",
    "            dfFinalItems.append(dfM)\n",
    "\n",
    "    dfFinal = pd.concat(dfFinalItems).reset_index(drop=True)\n",
    "    return dfFinal\n",
    "\n",
    "%matplotlib\n",
    "def plotDf(V, df, NoOfFig = 15, PlotsPerRow = 6):\n",
    "    LambdaColNames = [f\"a({x})_{v}\" for v in range(1, V+1) for x in range(0, v+1)]\n",
    "    lambdas = df[LambdaColNames].copy().astype(np.float32)\n",
    "    #plt.plot(lambdas.to_numpy()[0, 0:100], [x for x in range(0, 100)])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    rowOffset = 0\n",
    "    rowSkip = 1\n",
    "\n",
    "    if len(df) < NoOfFig:\n",
    "        NoOfFig = len(df)\n",
    "\n",
    "    TotalPlotsRow = math.ceil(NoOfFig/PlotsPerRow)\n",
    "\n",
    "    for plotNo in range (1, NoOfFig + 1):\n",
    "        ax = fig.add_subplot(TotalPlotsRow, PlotsPerRow, plotNo)\n",
    "        offset = 0\n",
    "        for v in range (1, V+1):\n",
    "            ax.plot([x for x in range(0, v+1)], lambdas.to_numpy()[(plotNo-1) * rowSkip + rowOffset, offset:offset+v+1])\n",
    "            offset = offset + v + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Row data and training data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_mongo(query = {\"stat.metadata.min_no_of_events_per_state\" : { \"$gte\": 100 }})\n",
    "print(f\"df len = {len(df)}\")\n",
    "\n",
    "V = 20\n",
    "print(f\"Loading available traffic classes available in system V={V}\")\n",
    "colClasses = getClasses(df, V)\n",
    "print(f\"{len(colClasses)} traffic classes is available\")\n",
    "print(\"Dropping classes with arrival E²ð² = 1 (gamma distribution, where E²= ð² -> lambda distribution)\")\n",
    "colClasses = dropClassesExact(colClasses, arrival_e2d2 = 1)\n",
    "print(f\"{len(colClasses)} traffic classes is available\")\n",
    "\n",
    "print(f\"Preparation of final statistics (for system with capacity V=1, ..., {V})\")\n",
    "dfFinal = prepareResults(V, colClasses, df)\n",
    "print(f\"{len(dfFinal)} systems is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDf(V, dfFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Flow Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "\n",
    "print(f\"Tensor Flow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/Output and Training/Verification data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputColNames(V, SR = 0, InputPar = ['a', 's']):\n",
    "    if SR == 0: SR = V\n",
    "    return [f\"{l}({x})_{v}\" for l in InputPar for v in range(1, V+1) if v%SR == 0 for x in range(0, v+1)]\n",
    "\n",
    "def getLabelColNames(V, SR = 0, LabelPar = ['a']):\n",
    "    if SR == 0: SR = V\n",
    "    return [f\"{l}({x})_{v}\" for l in LabelPar for v in range(1, V) if v%SR !=0 for x in range(0, v+1)]\n",
    "\n",
    "def getSubLabelColNames(V, LabelPar = ['a']):\n",
    "    return [f\"{l}({x})_{V}\" for l in LabelPar for x in range(0, V+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputColNames = getInputColNames(V)\n",
    "labelColNames = getLabelColNames(V)\n",
    "#print(\"inputColNames:\", inputColNames)\n",
    "#print(\"labelColNames:\", labelColNames)\n",
    "\n",
    "input = dfFinal[inputColNames].copy().astype(np.float32)\n",
    "label = dfFinal[labelColNames].copy().astype(np.float32)\n",
    "\n",
    "print(f\"Input shape {input.shape}\")\n",
    "print(f\"Label shape {label.shape}\")\n",
    "\n",
    "input.head()\n",
    "#plt.plot(input.to_numpy()[0:17,0:6])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_test, label_train, label_test = train_test_split(input, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubsetModel(dfTraining, V, functionsCalculateLen, SR = 0):\n",
    "    if SR == 0: SR = V\n",
    "    inputColNames  = getInputColNames(V, SR)\n",
    "    InpLayer = tf.keras.layers.Input(shape=(len(inputColNames),),name=\"input\")\n",
    "    input_train = dfTraining[inputColNames].copy().astype(np.float32)\n",
    "\n",
    "    LvlLayers = []\n",
    "    LvlLayers.append([])\n",
    "    for ln in range(0, len(functionsCalculateLen)):\n",
    "        LvlLayers.append([])\n",
    "    LvlLayers.append([])\n",
    "    \n",
    "    for vo in range(1,V):\n",
    "        #labelColNames  = [f\"{l}({x})_{vo}\" for l in ['a'] for x in range(0, vo+1)]\n",
    "        labelColNames = getSubLabelColNames(vo)\n",
    "        print(f\"v = {vo}: {labelColNames}\")\n",
    "        label_train = dfTraining[labelColNames].copy().astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "        LvlLayers[0].append(tf.keras.layers.Dense(len(inputColNames), name=f\"V_{vo}.layer_1\")(InpLayer))\n",
    "        \n",
    "        for idx, function in enumerate(functionsCalculateLen):\n",
    "            LvlLayers[idx+1].append(tf.keras.layers.Dense(function(vo), name=f\"V_{vo}.layer_{idx+2}\")(LvlLayers[idx][vo-1]))\n",
    "\n",
    "        LvlLayers[-1].append(tf.keras.layers.Dense(vo+1, name=f\"V_{vo}.layer_{len(functionsCalculateLen) + 2}\")(LvlLayers[-2][vo-1]))\n",
    "\n",
    "        tmpModel = tf.keras.Model(inputs=InpLayer,outputs=LvlLayers[-1][vo-1])\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "        optimizer = tf.keras.optimizers.Adam()#learning_rate=0.0002)\n",
    "        tmpModel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "        tmpModel.fit(input_train.to_numpy(), label_train.to_numpy(), epochs=20, batch_size=1, verbose=1)\n",
    "\n",
    "    OutLayer = tf.keras.layers.Concatenate(name=\"Output\")(LvlLayers[-1]) if len(LvlLayers[-1]) > 0 else LvlLayers[-1]\n",
    "    model = tf.keras.Model(inputs=InpLayer,outputs=OutLayer)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def createModel(dfTraining, V, SR = 0):\n",
    "    inputColNames  = getInputColNames(V, SR)\n",
    "    labelColNames = getLabelColNames(V, SR)\n",
    "    input_train = dfTraining[inputColNames].copy().astype(np.float32)\n",
    "    label_train = dfTraining[labelColNames].copy().astype(np.float32)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(input.shape[1]),\n",
    "        tf.keras.layers.Dense(540, activation='relu'),\n",
    "        tf.keras.layers.Dense(2160, activation='relu'),\n",
    "        tf.keras.layers.Dense(540, activation='relu'),\n",
    "        tf.keras.layers.Dense(label.shape[1]),\n",
    "        ])\n",
    "\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(input_train.to_numpy(), label_train.to_numpy(), epochs=30)\n",
    "    return model\n",
    "\n",
    "def verifyModel(df, model, V):\n",
    "    inputColNames = inputColNames  = getInputColNames(V)\n",
    "    labelColNames = getLabelColNames(V)\n",
    "\n",
    "    input = df[inputColNames].copy().astype(np.float32)\n",
    "    label = df[labelColNames].copy().astype(np.float32)\n",
    "    model.evaluate(input, label, verbose=2)\n",
    "    pr_label_data = model.predict(input)\n",
    "    pr_label = pd.DataFrame(columns = labelColNames, data = pr_label_data, index=input.index)\n",
    "    print(f\"input type {type(input)}, pr_label type {type(pr_label)}\")\n",
    "    dfAprox = pd.concat([input, pr_label], axis=1)\n",
    "    plotDf(V, dfAprox, 10, 5)\n",
    "    \n",
    "\n",
    "#def SaveAllModels(V, distrName, startRow):\n",
    "#    occDistr = LoadData(distrName, V, startRow)\n",
    "#    for v in range(2,V+1):\n",
    "#        result = CreateModel(v, v, occDistr)\n",
    "#        result.save(f\"./trained_models/{distrName}/gamma/model_all_points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Preparation of training and verification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTraining = dfFinal.sample(frac = 0.75);\n",
    "dfVerification = dfFinal.drop(dfTraining.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant with single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel(dfTraining, V)\n",
    "verifyModel(dfVerification, model, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant with many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functionsCalculateLen = [\n",
    "    lambda x: x*2,\n",
    "    lambda x: x*15,\n",
    "    lambda x: x*3,\n",
    "]\n",
    "model = createSubsetModel(dfTraining, V, functionsCalculateLen)\n",
    "\n",
    "verifyModel(dfVerification, model, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
